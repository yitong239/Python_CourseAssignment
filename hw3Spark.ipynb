{"cells":[{"cell_type":"code","source":["#Part B - Alternate - Structured Streaming \n#Come up with a set of at least 10 SQL questions that involve joins, order by, group by and aggregate statements and implement them on your data using Spark SQL.\nem= spark.read.csv('/FileStore/tables/data.csv', header = True, inferSchema = True)\nfeatures=spark.read.csv('/FileStore/tables/Features.csv', header = True, inferSchema = True)\nfeatures = features.selectExpr(\"Fuel_Price as price\",\"Store as Store\",\"CPI as CPI\",\"Unemployment as Unemployment\",\"IsHoliday as IsHoliday\",\"Temperature as Temperature\")\nsales=spark.read.csv('/FileStore/tables/sales.csv', header = True, inferSchema = True)\nstores=spark.read.csv('/FileStore/tables/stores.csv', header = True, inferSchema = True)\nfeatures=features.drop('MarkDown1')\nfeatures=features.drop('MarkDown2')\nfeatures=features.drop('MarkDown3')\nfeatures=features.drop('MarkDown4')\nfeatures=features.drop('MarkDown5')\n\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#Compare the avg sale in holidy with in normal days.\n\nsales.filter(sales.IsHoliday==\"FALSE\").agg({\"Weekly_Sales\": \"avg\"}).collect()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#Compare the avg sale in holidy with in normal days.\n\nsales.filter(sales.IsHoliday==\"TRUE\").agg({\"Weekly_Sales\": \"avg\"}).collect()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.functions import col\nem =em.select([col(c).cast(\"double\").alias(c) for c in em.columns])\nstores =stores.select([col(c).cast(\"double\").alias(c) for c in stores.columns])\nsales =sales.select([col(c).cast(\"double\").alias(c) for c in sales.columns])\nfeatures =features.select([col(c).cast(\"double\").alias(c) for c in features.columns])\n\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["em.count"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["raw_data=em.rdd\nprint(raw_data)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql import SQLContext\ndata = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/data.csv\")\ndata.cache() # Cache data for faster reuse\ndata = data.dropna() # drop rows with missing value\ntype(data)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#find out the outlook of customer purchase quantity\ndisplay(em.select('Quantity').groupBy('Quantity').count().orderBy(\"count\",ascending=False))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#find out the first 5 of product whose price is more than 10\ndisplay(em.filter(em.UnitPrice>10).limit(5))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#find out the most quantity\ndisplay(em.select('Quantity','Description').orderBy('Quantity',ascending=False))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#In this data set there is no prodct name,so we can search the total number of one product purchase based on the descripition.\ndisplay(em.select('Description').groupBy('Description').count())"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#Since there are three seperated dataset of store sales,we can use join to view these tables and merge them.\ndisplay(stores.filter(stores.Size>100000).join(sales,sales.Store==stores.Store))\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(stores.join(features,features.Store==stores.Store))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#find out the most expemsive one\nem.agg({\"UnitPrice\": \"max\"}).collect()\n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#create a temporary view table\nem.createOrReplaceTempView('tmpTable')\ndisplay(spark.read.table('tmpTable'))\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["spark.sql(\"select * from data_csv\").show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#MLlib and Machine Learning;Apply a MLlib and Machine Learning analysis of your project data\nfrom pyspark.sql.functions import col  # for indicating a column using a string in the line below\ndf=features\ndf = df.select([col(c).cast(\"double\").alias(c) for c in df.columns])\ndf=df.drop(\"date\")\ndf.printSchema()\n\n"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["df=df.na.drop(subset=[\"CPI\"])\ndf=df.na.drop(subset=[\"Unemployment\"])\ndf.where(col(\"Unemployment\").isNull())\ndf.where(col(\"CPI\").isNull())\n"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["df.na.drop()\ntrain, test = df.randomSplit([0.7, 0.3])\nprint \"We have %d training examples and %d test examples.\" % (train.count(), test.count())"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["display(df)\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(train.select(\"CPI\", \"price\"))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler, VectorIndexer\nfeaturesCols = df.columns\n\nfeaturesCols.remove(\"price\")\n# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\nvectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")\n# This identifies categorical features and indexes them.\nvectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["type(vectorAssembler )"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["from pyspark.ml.regression import GBTRegressor\n# Takes the \"features\" column and learns to predict \"price\"\ngbt = GBTRegressor(labelCol=\"price\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["gbt\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\n# Define a grid of hyperparameters to test:\n#  - maxDepth: max depth of each decision tree in the GBT ensemble\n#  - maxIter: iterations, i.e., number of trees in each GBT ensemble\nparamGrid = ParamGridBuilder()\\\n  .addGrid(gbt.maxDepth, [2, 5])\\\n  .addGrid(gbt.maxIter, [10, 100])\\\n  .build()\n# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true labels with predictions.\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol())\n# Declare the CrossValidator, which runs model tuning for us.\ncv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["train"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])\npipelineModel = pipeline.fit(train)\n"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["predictions = pipelineModel.transform(test)\ndisplay(predictions.select(\"price\", \"prediction\", *featuresCols))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["display(predictions.select(\"price\", \"prediction\"))"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["#Part D GraphX and GraphFrames\n#A dataframe is created for the retail sale and purchase of product from one location to another. Here are the vertices:\nvertices = sqlContext.createDataFrame([\n(\"1\",\"US\",24.4039811),\n(\"2\",\"Canada\",164.4110262),\n(\"3\",\"Mexico\",60.75008053),\n(\"4\",\"S. & Cent. America\",177.4399579),\n(\"5\",\"Europe\",17.63899156),\n(\"6\",\"Russia\",273.9722708),\n(\"7\",\"Other CIS\",81.69126071),\n(\"8\",\"Iraq\",177.4754949),\n(\"9\",\"Kuwait\",103.2976087),\n(\"10\",\"Saudi Arabia\",375.3165213),\n(\"11\",\"UAE\",123.2311992),\n(\"12\",\"Other Middle East\",203.159359),\n(\"13\",\"North Africa\",58.17278392),\n(\"14\",\"West Africa\",216.460312),\n(\"15\",\"East & S. Africa\",6.884463733),\n(\"16\",\"Australasia\",9.43056574),\n(\"17\",\"China\",2.885958344),\n(\"18\",\"India\",0.004605561),\n(\"19\",\"Japan\",1.80022),\n(\"20\",\"Singapore\",0.102406838),\n(\"21\",\"Other Asia Pacific\",41.02723453)], [\"id\", \"country\", \"total_value\"])"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["edges = sqlContext.createDataFrame([\n(\"1\",\"2\",\"selling\"),\n(\"1\",\"3\",\"selling\"),\n(\"1\",\"4\",\"selling\"),\n(\"1\",\"8\",\"purchase\"),\n(\"1\",\"9\",\"selling\"),\n(\"1\",\"10\",\"purchase\"),\n(\"1\",\"14\",\"selling\"),\n(\"1\",\"19\",\"purchase\"),\n(\"1\",\"20\",\"selling\"),\n(\"1\",\"21\",\"purchase\"),\n(\"2\",\"3\",\"selling\"),\n(\"2\",\"4\",\"purchase\"),\n(\"2\",\"6\",\"selling\"),\n(\"2\",\"7\",\"purchase\"),\n(\"2\",\"8\",\"selling\"),\n(\"2\",\"14\",\"selling\"),\n(\"2\",\"19\",\"purchase\"),\n(\"2\",\"21\",\"selling\"),\n(\"3\",\"5\",\"purchase\"),\n(\"3\",\"8\",\"selling\"),\n(\"3\",\"14\",\"purchase\"),\n(\"3\",\"15\",\"purchase\"),\n(\"4\",\"5\",\"selling\"),\n(\"4\",\"11\",\"purchase\"),\n(\"4\",\"12\",\"selling\"),\n(\"5\",\"13\",\"purchase\"),\n(\"5\",\"14\",\"purchase\"),\n(\"6\",\"7\",\"selling\"),\n(\"6\",\"9\",\"purchase\"),\n(\"6\",\"19\",\"purchase\"),\n(\"6\",\"20\",\"selling\"),\n(\"7\",\"8\",\"purchase\"),\n(\"7\",\"10\",\"purchase\"),\n(\"8\",\"12\",\"selling\"),\n(\"8\",\"14\",\"purchase\"),\n(\"9\",\"16\",\"purchase\"),\n(\"9\",\"18\",\"purchase\"),\n(\"10\",\"20\",\"purchase\"),\n(\"10\",\"21\",\"selling\"),\n(\"12\",\"13\",\"purchase\"),\n(\"13\",\"17\",\"purchase\"),\n(\"14\",\"17\",\"selling\"),\n(\"15\",\"16\",\"purchase\"),\n(\"15\",\"19\",\"selling\"),\n(\"16\",\"17\",\"purchase\"),\n(\"17\",\"20\",\"purchase\"),\n(\"18\",\"20\",\"selling\"),\n(\"18\",\"21\",\"purchase\"),\n(\"19\",\"20\",\"selling\"),\n(\"20\",\"21\",\"purchase\")],[\"src\", \"dst\", \"relationship\"])"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["import sys\nprint(sys.path)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["from graphframes import *\ng = GraphFrame(vertices, edges)\nprint g\n"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["paths = g.find(\"(a)-[e]->(b)\")\\\n  .filter(\"e.relationship = 'export'\")\\\n  .filter(\"a.total_value < b.total_value\")\n# The `paths` variable contains the vertex information, which we can extract:\ne2 = paths.select(\"e.src\", \"e.dst\", \"e.relationship\")\n\n# In Spark 1.5+, the user may simplify the previous call to:\n# val e2 = paths.select(\"e.*\")\n\n# Construct the subgraph\ng2 = GraphFrame(g.vertices, e2)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["#'g' is graph created from those vertices and edges\ndisplay(g.vertices)\n\n"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["display(g.edges)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["display(g.degrees)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["display(g.outDegrees)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["display(g.inDegrees)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["min = g.vertices.groupBy().min(\"total_value\")\ndisplay(min)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["max = g.vertices.groupBy().max(\"total_value\")\ndisplay(max)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["display(g2.vertices)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["result = g.stronglyConnectedComponents(maxIter=10)\ndisplay(result.select(\"id\", \"component\"))"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["results = g.shortestPaths(landmarks=[\"1\", \"6\"])\ndisplay(results)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["results = g.pageRank(resetProbability=0.15, tol=0.01)\ndisplay(results.vertices)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["display(results.edges)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":48}],"metadata":{"name":"test","notebookId":1672142189926616},"nbformat":4,"nbformat_minor":0}
